{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A beginner's look at scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confession** : The title is somewhat misleading.\n",
    "\n",
    "We'll be doing much more than look at the [scikit-learn tutorials](http://scikit-learn.org/stable/tutorial/index.html). My purpose is to initiate _exploration_ and _understanding_ of the underlying ideas found in this machine learning library. For brevity, I assume the reader is already familiar with basic machine learning concepts (e.g. supervised vs unsupervised learning) and won't spend time talking about how major algorithms work. Far more important, I think, is getting a feel for the workflow and seeing what options are available to you. \n",
    "\n",
    "So consider this less of a tutorial and more like an experiment. I write as if the reader has roughly the same background as me (that is to say, informal dabbling). We'll take detours, supply more information to details, and play around with features. In fact, that's exactly what I did when writing this page up...since I'm also new to IPython. With that said, let's start!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Example: Irises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the \"Hello, World!\" of classification datasets: Fisher's irises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Fisher's iris dataset -- like the one in R\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit includes a few default datasets. Unlike R (for those familiar with the language), python's datasets are actually dictionaries. Use tab-completion to see the attributes for _iris_. As you might have guessed, DESCR provides us a description of the data set along with [other information.](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: K-Nearest Neighors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, time for some actual machine learning! This dataset is in two parts: the data containing the sepal and \n",
    "petal measurements, and the labels telling us what flower each observation is. These are found in the _data_ and\n",
    "_target_ attributes, respectively. So let's load them up and split our observations into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.6  3.   4.1  1.3]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 4.6  3.2  1.4  0.2]] [1 1 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n",
    "\n",
    "# Returns a permutation of the 150 instances\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "\n",
    "X_train = iris_X[indices[:-10]]\n",
    "y_train = iris_Y[indices[:-10]]\n",
    "\n",
    "X_test = iris_X[indices[-10:]]\n",
    "y_test = iris_Y[indices[-10:]]\n",
    "\n",
    "print(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so we're ready to train our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10\n",
      "Percent correct: 90.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Give your classifier the training data along with correct labels\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric = 'minkowski', \n",
    "                     metric_params = None, n_neighbors = 5, p = 2, weights = 'uniform')\n",
    "\n",
    "n = len(X_test)\n",
    "predict = knn.predict(X_test)\n",
    "correct = sum(predict == y_test)\n",
    "\n",
    "print('n = {0}'.format(n))\n",
    "print('Percent correct: {0:.2f} %'.format(100 * correct / n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a first try. So let's figure out what we just did. \n",
    "\n",
    "We assign a classifier to the variable _knn_. As an estimator object, _knn_ implements the fit() and predict() methods, which we use train and test our sets, respectively. The general flow we have thus far is this:\n",
    "\n",
    "1. Load our libraries and data\n",
    "2. Process and split our datasets into training, test sets, etc.\n",
    "3. Create a classifier object\n",
    "4. Give our classifier the training data ('Fit' the estimator)\n",
    "5. Run the classifier (Train)\n",
    "6. Predict our test sets\n",
    "7. Get results\n",
    "\n",
    "Pretty simple, right? Of course, the bulk of the work has been done for us in preparing the dataset. In the real world, getting data in the right form (aka data wrangling) will preoccupy most of our time. Nonetheless, it is extremely important to understand how to tweak our models to get better performance. So let's talk about the parameters we gave to the classifier.\n",
    "\n",
    "Parameter _algorithm_ has four options:\n",
    "\n",
    "- 'brute' for a brute force search\n",
    "- 'ball_tree' for constructing a ball tree\n",
    "- 'kd_tree' for constructing a kd-tree\n",
    "- 'auto' for the lazy ;) \n",
    "\n",
    "In all seriousness, 'auto' lets the model decide which implementation will work best given the training set you passed into the fit method. To read about the algorithms in semi-detail, consult the [user guide.](http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms) As an aside, the user guide is _very_ helpful and will aid your decision-making on parameter choice. \n",
    "\n",
    "Next, leaf_size. The rule of thumb is that a higher leaf_size uses less memory and makes tree construction faster (assuming no brute force). The tradeoff is query time, which by default is set to 30. \n",
    "\n",
    "Moving on...metrics. Scikit handles its metrics in a class called, you guessed it, [DistanceMetric](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html). Notice that we could've chosen any one of those metrics in the real-valued vector spaces table. Later, we'll experiment to see if this improves or weakens our predictive accuracy. Now, for those unfamiliar with the Minkowski metric, we present it below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ X = (x_{1}, x_{2}, ... , x_{n}), Y = (y_{1}, y_{2}, ..., y_{n}) $ be vectors in $\\mathbb{R}^{n}$.\n",
    "Then the $\\textbf{Minkowski distance}$ of $X$ and $Y$ is:\n",
    "   \n",
    "$$ d(X, Y) = \\Big(\\sum_{i = 1}^{n} |x_{i} - y_{i}|^{p} \\Big)^{1/p} $$\n",
    "\n",
    "In the above classifier, we set $p = 2$, which is the equivalent to the Euclidean metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _n_neighbors_ parameter is pretty straightforward. This is the number of neighbors we compare our point to when predicting its label. By default, we look at the 5 closest neighbors. However, this leads us to our next issue: do we simply tally up the labels of a point's neighbors and go with a majority vote? Or, do we weight the closest neighbor more highly than the fifth closest neighbor? This is what the _weights_ parameter handles. For majority vote, go with 'uniform', and for weighing points by closeness, go with 'distance'. If you're clever, you can even make up your own rule and pass it in as a callable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
